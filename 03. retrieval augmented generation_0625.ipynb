{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b22d000",
   "metadata": {},
   "source": [
    "## RAG\n",
    "- hallucination 환각 문제 발생 \n",
    "- 거대언어의 문제점: 최신 정보의 부재\n",
    "- -> 극복방안: 분야의 제한없이 광범위한 주제의 질문을 받고 답변하는 질의응답 시스템을 응용해서 RAG\n",
    "- 거대언어 이전: Q&A쌍을 만들어 둠. \n",
    "  - 쿼리 들어오면 제일 유사한 쿼리를 찾아냄. ex) 코사인 유사도이용 후 tf-idf를 이용한 벡터형으로 변환하여 저장."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23021d8e",
   "metadata": {},
   "source": [
    "### ODQA(Open Domain Query Answering)\n",
    "  - 1단계: 위키피디어와 같은 대량의 문서들을 수집하고 인덱싱해서 지식베이스에 저장.최근에는 딥러닝기법을 이용해 각 문서를 벡터로 임베딩. 문서간 유사도가 잘 표현할 수 있도록 임베딩 방법을 학습.\n",
    "  - 2단계: 질의가 들어오면 임베딩하고 지식베이스에 임베딩된 문서들과 유사도를 비교해서 응답이 있을만한 문서를 검색.\n",
    "  - 3단계: 질의와 검색된 문서를 학습된 언어모델에 입력하고 문서로부터 응답을 추출. \n",
    "\n",
    "### RAG(Retrieval-Augmented Generation): 검색된 정보를 바탕으로 언어 모델이 텍스트를 생성하는 방식으로 작동\n",
    "   - 수집한 데이터를 일정 단위(문서 혹은 그보다 작은단위)로 임베딩(Document Indexing).\n",
    "   - 쿼리가 들어오면 임베딩하고 수집된 데이터 임베딩과 유사도를 비교해 가장 유사한 데이터 검색(Retriever).\n",
    "   - 쿼리 + 데이터(문서)로 프롬프트를 작성해 LLM으로부터 결과를 받음(Generator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324325a6",
   "metadata": {},
   "source": [
    "## RAG 구성 요소\n",
    "- Document loaders: 대상 문서 읽어오기\n",
    "- Text Splitting: 문서를 chunk 단위로 splitting\n",
    "- Text Embedding: 벡터로 임베딩\n",
    "- Vector Stores: 벡터(+원본) 저장\n",
    "- Retrievers: 쿼리에 대해 유사벡터 / 문서검색 및 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b448a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea47577",
   "metadata": {},
   "source": [
    "#### 문서 loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f6d2f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading state_of_the_union.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain_community\\document_loaders\\text.py\u001b[0m in \u001b[0;36mlazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xe2 in position 523: illegal multibyte sequence",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8940\\3150778038.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state_of_the_union.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Document 객체 리스트를 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain_core\\document_loaders\\base.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;34m\"\"\"Load data into Document objects.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32masync\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\langchain_community\\document_loaders\\text.py\u001b[0m in \u001b[0;36mlazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error loading {self.file_path}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Error loading {self.file_path}\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading state_of_the_union.txt"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# 문서들을 읽어오기 위한 라이브러리. 이후 처리를 위해 문자열의 리스트 형태로 읽어들임\n",
    "# LangChain은 부가적으로 필요한 메타정보를 추가하여 Document객체로 저장\n",
    "\n",
    "#url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/ modules/state_of_the_union.txt\"\n",
    "#res = requests.get(url)\n",
    "#with open(\"state_of_the_union.txt\", \"w\") as f:\n",
    "#    f.write(res.text)\n",
    "    \n",
    "loader = TextLoader('state_of_the_union.txt')\n",
    "documents = loader.load() # Document 객체 리스트를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a6ca4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "404: Not Found\n"
     ]
    }
   ],
   "source": [
    "print(len(documents[0].page_content))\n",
    "print(documents[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e5aed",
   "metadata": {},
   "source": [
    "#### text splitting 종류 \n",
    "- Character Splitting: 문서를 n개의 문자 단위로 split.\n",
    "- Recursive Character Text Splitting: chunk size를 넘지 않는 범위에서 separator 기준으로 분리.\n",
    "- Document Specific Splitting: Level 2를 기반으로 하여, 문서의 종류에 따라 특성에 맞게 각기 다른 separator 적용.\n",
    "- Semantic Splitting: 하나의 Chunk가 의미적으로 최대한 유사하도록 split. 유사도를 계산함.\n",
    "- Agentic Splitting: 첫 문장의 주제 파악 후 현재 문장의 주제를 파악. 이전 문장의 주제와 비교해서 동일하면 chunk 유지하며 진행. 주제가 달라지면 새로운 chunk 생성. 2로 돌아가서 마지막 문장까지 반복\n",
    "- 보통 2를 제일 많이 사용하는 듯? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "990b23a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# RecursiveCharacterTextSplitter: 객체 생성, 청크의 최대 크기(500), 청크가 겹치는 부분의 크기(20) 지정\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size = 500,\n",
    "                                                    chunk_overlap = 20)\n",
    "\n",
    "# 텍스트 분할 \n",
    "\n",
    "chunks = recursive_splitter.split_text(documents[0].page_content)\n",
    "\n",
    "print(len(chunks)) # chunk 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d3bccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14 [14]\n"
     ]
    }
   ],
   "source": [
    "# 각 청크의 크기 확인 \n",
    "\n",
    "chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "\n",
    "print(max(chunk_sizes), min(chunk_sizes), chunk_sizes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5627f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[Document(page_content='404: Not Found', metadata={'source': './state_of_the_union.txt'})]\n"
     ]
    }
   ],
   "source": [
    "docs = recursive_splitter.split_documents(documents)\n",
    "print(len(docs))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd1b2f",
   "metadata": {},
   "source": [
    "#### Text Embedding\n",
    "- 텍스트 데이터를 숫자 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37adc707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1536 1536 [0.0032154050918757333, -0.00967098752427458, -0.009704016878696898, -0.03255370564160612, 0.00018114522014075682, 0.026582003949986163, -0.013859105939734414, -0.006737983179879034, -0.021019864390557856, -0.03622656612807778]\n"
     ]
    }
   ],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings: OpenAIEmbeddings를 이용해 두개의 텍스트를 임베딩하고 결과 확인\n",
    "\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embed = OpenAIEmbeddings(model = 'text-embedding-ada-002',\n",
    "                         openai_api_key = OPENAI_API_KEY\n",
    "                        )\n",
    "\n",
    "texts = ['this is the first chunk of text','then another second chunk of text is here']\n",
    "\n",
    "res = embed.embed_documents(texts) # 문서 임베딩 결과 저장\n",
    "\n",
    "print(len(res), len(res[0]), len(res[1]), res[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9143b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  pass\n",
      "C:\\Users\\82103\\anaconda3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ea98d6584046968c08d4c4cb137b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\82103\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757b271583194057943dbb1346147f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0194ec146e40358cf3f6b9f8e0ccf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54760d6f5d4b411397d66363bf07ee1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c433afa7d346628c1fd01a4a4aab0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2dc0e59ff64fbdad3eebab62944285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393fbe1fc71942cfa13d9c03ddfc413e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13bf8b52b4b42a785cf0ca1768e2d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a291ecc60375452abd1c4bf4fbfd41f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba00245422ea4be38480b131dda299af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11ef86d896a428fa31b758c87bbdd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 384 384 [-0.011914781294763088, 0.0891803726553917, 0.03824286907911301, 0.01908119209110737, 0.059775590896606445, 0.0053163859993219376, 0.03878144174814224, -0.008798955008387566, 0.06023487076163292, -0.015470323152840137]\n"
     ]
    }
   ],
   "source": [
    "# Sentence Transformers on Hugging Face: BERT를 기반으로 하여 Sentence를 임베딩하도록 학습한 모델\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embed = SentenceTransformerEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "\n",
    "texts = ['this is the first chunk of text', 'then another second chunk of text is here']\n",
    "\n",
    "res = embed.embed_documents(texts) # 문서 임베딩 결과 저장 \n",
    "\n",
    "print(len(res), len(res[0]), len(res[1]), res[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f068a",
   "metadata": {},
   "source": [
    "### Vector Store\n",
    "- 임베딩 벡터 저장\n",
    "- query에 대해 가장 유사한 벡터를 반환하는 retrieve 기능을 함께 제공\n",
    "- query와 유사한 문서의 내용을 store에 검색해서 이를 바탕으로 답변을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ad02716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\.cache\\chroma\\onnx_models\\all-MiniLM-L6-v2\\onnx.tar.gz: 100%|█████| 79.3M/79.3M [00:22<00:00, 3.70MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id1', 'id2']], 'distances': [[0.7111214399337769, 1.0109773874282837]], 'metadatas': [[{'source': 'my_source1'}, {'source': 'my_source2'}]], 'embeddings': None, 'documents': [['This is a document', 'This is another document']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# chroma 라이브러리 이용 - 벡터 스토어 생성 및 관리, 유사도 검색, 확장성\n",
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client() # 클라이언트 생성 \n",
    "\n",
    "collection = chroma_client.create_collection(name = \"my_collection\") # collection: 벡터를 저장하고 관리하는 논리적 단위 생성.\n",
    "\n",
    "# 문서 추가 \n",
    "collection.add(\n",
    "    documents = [\"This is a document\", \"This is another document\"],\n",
    "    metadatas = [{\"source\": \"my_source1\"}, {\"source\": \"my_source2\"}],\n",
    "    ids = [\"id1\", \"id2\"]\n",
    ")\n",
    "\n",
    "# collection query: 유사한 문서 검색\n",
    "results = collection.query( query_texts=[\"This is a query document\"],\n",
    "                           n_results=2\n",
    "                          )\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db413b",
   "metadata": {},
   "source": [
    "- ids: 쿼리된 문서들의 고유 ID를 포함하는 리스트\n",
    "- distances: 쿼리된 문서들과 입력 쿼리 문서 간의 유사도 점수(거리). 거리 값이 작을수록 두 문서가 유사함\n",
    "- metadatas: 쿼리된 문서들의 메타데이터\n",
    "- embeddings: 쿼리된 문서들의 임베딩 벡터를 포함하는 리스트\n",
    "- documents: 쿼리된 문서들의 실제 텍스트 내용을 포함하는 리스트\n",
    "- uris: 문서들의 URI(Uniform Resource Identifier)를 포함하는 리스트\n",
    "- data: 추가적인 데이터를 포함하는 리스트\n",
    "- included: 쿼리 결과에 포함된 필드를 나타내는 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd6666bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id0']], 'distances': [[1.7565205097198486]], 'metadatas': [[{'source': './state_of_the_union.txt'}]], 'embeddings': None, 'documents': [['404: Not Found']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# state_of_the_union예제\n",
    "collection = chroma_client.create_collection(name = \"state_of_the_union\")\n",
    "\n",
    "ids = ['id'+str(i) for i in range(len(docs))]\n",
    "doc_texts = [docs[i].page_content for i in range(len(docs))]\n",
    "doc_metadatas = [docs[i].metadata for i in range(len(docs))]\n",
    "collection.add(documents = chunks,\n",
    "               metadatas = doc_metadatas,\n",
    "               ids = ids\n",
    "              )\n",
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts = [\"What did the president say about Ketanji Brown Jackson\"],\n",
    "    n_results = 1\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63cac554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 page_content='404: Not Found' metadata={'source': './state_of_the_union.txt'}\n"
     ]
    }
   ],
   "source": [
    "# LangChain의 Chroma 라이브러리 사용 \n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, embed) \n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "query_result = db.similarity_search(query)\n",
    "print(len(query_result), query_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3516702",
   "metadata": {},
   "source": [
    "- Chroma와 같은 벡터 스토어를 사용할 때, 데이터를 로드할 때 임베딩 함수를 지정해야 한다는 것은,텍스트 데이터를 벡터로 변환하는 함수가 필요하다는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0b7a5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='404: Not Found' metadata={'source': './state_of_the_union.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Chroma 벡터 스토어 생성 및 데이터 저장 \n",
    "db2 = Chroma.from_documents(docs, embed, persist_directory = \"./chroma_db\")\n",
    "\n",
    "# 디스크에서 벡터 스토어 로드\n",
    "db3 = Chroma(persist_directory = \"./chroma_db\", embedding_function = embed)\n",
    "\n",
    "# 유사도 검색 수행\n",
    "docs = db3.similarity_search(query)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39506581",
   "metadata": {},
   "source": [
    "### Retriever를 이용한 Q&A 구현\n",
    "- Retriever\n",
    "  - 주어진 query를 벡터로 변환하고 vector store에서 유사도가 높은 벡터들을 반환.\n",
    "  - 일반적으로 vector store가 기능을 함께 제공.\n",
    "- Q&A 구현\n",
    "  - Retriever가 반환한 Context와 원래 query로 프롬프트를 생성하고 이를 LLM에 입력하여 답변을 생성.\n",
    "  - Context의 내용만 이용해 답변하도록 하는 프롬프트 템플릿이 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53f9edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Context만 이용해서 답변하도록 프롬프트 템플릿 생성\n",
    "\n",
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {query}\n",
    "Answer: \n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"context\", \"query\"],\n",
    "    template = template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc1aaa",
   "metadata": {},
   "source": [
    "- 추가로 LangChain hub: LLM의 효율적인 관리를 위해 제공되는 라이브러리\n",
    "  - RetrievalQA Chain: RAG 파이프라인 예제들\n",
    "  - Runnable PromptTemplate: 프롬프트 템플릿 예제들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "954b362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") # langchain hub에 저장된 rag prompt 사용\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4db611",
   "metadata": {},
   "source": [
    "- RAG를 위한 Chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d52e7b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know.\", response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 76, 'total_tokens': 81}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dffa9f30-4499-45de-88e9-090947f5f9d4-0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "# from langchain_core.runnables import RunnableLambda\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever() # vector store를 retriever로 사용\n",
    "openai = ChatOpenAI(\n",
    "    model_name ='gpt-3.5-turbo',\n",
    "    api_key = OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "rag_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()}\n",
    "             |prompt_template\n",
    "             |openai\n",
    "            )\n",
    "\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfdd27",
   "metadata": {},
   "source": [
    "- RunnableParallel: 두 개 이상의 Runnable 이 (sequential이아닌) parallel하게 실행되어야 하는 상황에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acf371fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11684\\55917146.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msetup_and_retrieval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunnableParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"context\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mretriever\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mRunnablePassthrough\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mchain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msetup_and_retrieval\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mprompt\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0moutput_parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# sequence = runnable_1 | runnable_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "setup_and_retrieval = RunnableParallel({\"context\":retriever,\"question\":RunnablePassthrough()})\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "# sequence = runnable_1 | runnable_2\n",
    "# 혹은 sequence = RunnableSequence(first=runnable_1, last=runnable_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61003fa0",
   "metadata": {},
   "source": [
    "- 출력이 AIMessage 객체가 아닌 문자열이 되도록 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fd83bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "rag_chain=({\"context\": retriever, \"query\": RunnablePassthrough()}\n",
    "           |prompt_template\n",
    "           |openai\n",
    "           |StrOutputParser()\n",
    "          )\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f6427",
   "metadata": {},
   "source": [
    "- 프롬프트 템플릿 대신 RetrievalQA 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62d3c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What did the president say about Justice Breyer',\n",
       " 'result': \"I'm sorry, but I do not have the specific information about what the president said about Justice Breyer.\"}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = openai,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = db.as_retriever()\n",
    ")\n",
    "\n",
    "query =\"What did the president say about Justice Breyer\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4df92a4",
   "metadata": {},
   "source": [
    "- RetrievalQAWithSourcesChain 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b561747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What did the president say about Justice Breyer',\n",
       " 'answer': \"I don't know.\\n\",\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = openai,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = db.as_retriever()\n",
    ")\n",
    "query = \"What did the president say about Justice Breyer\"\n",
    "qa_with_sources.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a4959a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
