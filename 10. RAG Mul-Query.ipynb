{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fdee39",
   "metadata": {},
   "source": [
    "## LangChain Multi-Query for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da519bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (file://C:/Users/sy/.cache/huggingface/datasets/jamescalam___json/jamescalam--ai-arxiv-chunked-c0ecde7e34f06e42/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12752\\101004357.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jamescalam/ai-arxiv-chunked\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m     \u001b[0mdownload_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1810\u001b[1;33m         \u001b[0mdownload_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstorage_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_split_generators_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepare_split_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m         \u001b[1;34m\"\"\"Get kwargs for `self._split_generators()` from `prepare_split_kwargs`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mprepare_split_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571b0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"hf://datasets/jamescalam/ai-arxiv-chunked/train.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8991742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>1</td>\n",
       "      <td>loss combining language modeling, distillation...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>2</td>\n",
       "      <td>in real-time has the potential to enable novel...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>3</td>\n",
       "      <td>through distillation via the supervision of a ...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>4</td>\n",
       "      <td>generalization capabilities of the model and h...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi  chunk-id                                              chunk  \\\n",
       "0  1910.01108         0  DistilBERT, a distilled version of BERT: small...   \n",
       "1  1910.01108         1  loss combining language modeling, distillation...   \n",
       "2  1910.01108         2  in real-time has the potential to enable novel...   \n",
       "3  1910.01108         3  through distillation via the supervision of a ...   \n",
       "4  1910.01108         4  generalization capabilities of the model and h...   \n",
       "\n",
       "           id                                              title  \\\n",
       "0  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "1  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "2  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "3  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "4  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  As Transfer Learning from large-scale pre-trai...   \n",
       "1  As Transfer Learning from large-scale pre-trai...   \n",
       "2  As Transfer Learning from large-scale pre-trai...   \n",
       "3  As Transfer Learning from large-scale pre-trai...   \n",
       "4  As Transfer Learning from large-scale pre-trai...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/1910.01108   \n",
       "1  http://arxiv.org/pdf/1910.01108   \n",
       "2  http://arxiv.org/pdf/1910.01108   \n",
       "3  http://arxiv.org/pdf/1910.01108   \n",
       "4  http://arxiv.org/pdf/1910.01108   \n",
       "\n",
       "                                             authors categories  \\\n",
       "0  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "1  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "2  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "3  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "4  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "\n",
       "                                             comment journal_ref  \\\n",
       "0  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "1  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "2  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "3  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "4  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "\n",
       "  primary_category  published   updated              references  \n",
       "0            cs.CL   20191002  20200301  [{'id': '1910.01108'}]  \n",
       "1            cs.CL   20191002  20200301  [{'id': '1910.01108'}]  \n",
       "2            cs.CL   20191002  20200301  [{'id': '1910.01108'}]  \n",
       "3            cs.CL   20191002  20200301  [{'id': '1910.01108'}]  \n",
       "4            cs.CL   20191002  20200301  [{'id': '1910.01108'}]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97566773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41584 entries, 0 to 41583\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   doi               41584 non-null  float64\n",
      " 1   chunk-id          41584 non-null  int64  \n",
      " 2   chunk             41584 non-null  object \n",
      " 3   id                41584 non-null  float64\n",
      " 4   title             41584 non-null  object \n",
      " 5   summary           41584 non-null  object \n",
      " 6   source            41584 non-null  object \n",
      " 7   authors           41584 non-null  object \n",
      " 8   categories        41584 non-null  object \n",
      " 9   comment           25617 non-null  object \n",
      " 10  journal_ref       2216 non-null   object \n",
      " 11  primary_category  41584 non-null  object \n",
      " 12  published         41584 non-null  int64  \n",
      " 13  updated           41584 non-null  int64  \n",
      " 14  references        41584 non-null  object \n",
      "dtypes: float64(2), int64(3), object(10)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be3b094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    doc = Document(\n",
    "        page_content=row[\"chunk\"],\n",
    "        metadata={\n",
    "            \"title\": row[\"title\"],\n",
    "            \"source\": row[\"source\"],\n",
    "            \"id\": row[\"id\"],\n",
    "            \"chunk-id\": row[\"chunk-id\"],\n",
    "            \"text\": row[\"chunk\"]\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea77f8d",
   "metadata": {},
   "source": [
    "#### Embedding and Vector DB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d14500ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8686adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sy\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaffee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ea5296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83ba38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5615e662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-multi-query-demo\"\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f449aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41584"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d40cb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5140ecb5a844ba59a9404760d808c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/416 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "    i_end = min(len(docs), i+batch_size)\n",
    "    docs_batch = docs[i:i_end]\n",
    "    # get IDs\n",
    "    ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
    "    # get text and embed\n",
    "    texts = [d.page_content for d in docs_batch]\n",
    "    embeds = embed.embed_documents(texts=texts)\n",
    "    # get metadata\n",
    "    metadata = [d.metadata for d in docs_batch]\n",
    "    to_upsert = zip(ids, embeds, metadata)\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c43a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sy\\anaconda3\\lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92ff0f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sy\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7de5a6",
   "metadata": {},
   "source": [
    "- MultiQueryRetriever 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e4ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04fcc5",
   "metadata": {},
   "source": [
    "#### logging 설정\n",
    "- 디버깅이나 모니터링을 위하여 소프트웨어 동작 상태 정보를 기록해서 볼 수 있음 \n",
    "- 로그(log)를 생성하도록 시스템을 작성하는 활동\n",
    "- 문제가 발생하는 경우 로그가 중요한 정보를 제공하기에 문제 진단, 해결가능\n",
    "- INFO: 상태 변경과 같은 정보성 로그를 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41347920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b830bad1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What information can you provide about llama 2?', 'What are some details about llama 2?', 'Can you share some insights on llama 2?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"tell me about llama 2?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfb019",
   "metadata": {},
   "source": [
    "- retriever.get_relevant_documents 질의와 관련된 문서를 검색하고 반환\n",
    "- 쿼리와 관련하여 검색기가 찾은 문서 수: 5개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7a59fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'chunk-id': 1.0, 'id': 2307.09288, 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'chunk-id': 9.0, 'id': 2307.09288, 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='models will be released as we improve model safety with community feedback.\\nLicense A custom commercial license is available at: ai.meta.com/resources/\\nmodels-and-libraries/llama-downloads/\\nWhere to send commentsInstructions on how to provide feedback or comments on the model can be\\nfound in the model README, or by opening an issue in the GitHub repository\\n(https://github.com/facebookresearch/llama/ ).\\nIntended Use\\nIntended Use Cases L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is intended for commercial and research use in English. Tuned models\\nare intended for assistant-like chat, whereas pretrained models can be adapted\\nfor a variety of natural language generation tasks.\\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade\\ncompliancelaws). UseinlanguagesotherthanEnglish. Useinanyotherway\\nthat is prohibited by the Acceptable Use Policy and Licensing Agreement for\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle.\\nHardware and Software (Section 2.2)\\nTraining Factors We usedcustomtraininglibraries, Meta’sResearchSuperCluster, andproductionclustersforpretraining. Fine-tuning,annotation,andevaluationwerealso', metadata={'chunk-id': 317.0, 'id': 2307.09288, 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='Q:Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\\nA:TheWar inVietnam was6months. Thegestationperiod forallama is11months, which ismore than 6\\nmonths. Thus, allama could notgive birth twice duringtheWar inVietnam. So the answer is no.\\nQ:Yes or no: Would a pear sink in water?\\nA:Thedensityofapear isabout 0:6g=cm3,which islessthan water.Objects lessdense than waterﬂoat. Thus,\\napear would ﬂoat. So the answer is no.\\nTable 26: Few-shot exemplars for full chain of thought prompt for Date Understanding.\\nPROMPT FOR DATE UNDERSTANDING\\nQ:2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\\nA:If2015 iscomingin36hours, then itiscomingin2days. 2days before01/01/2015 is12/30/2014, sotoday\\nis12/30/2014. Sooneweek from todaywillbe01/05/2015. So the answer is 01/05/2015.', metadata={'chunk-id': 137.0, 'id': 2201.11903, 'source': 'http://arxiv.org/pdf/2201.11903', 'title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'}),\n",
       " Document(page_content='Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 .', metadata={'chunk-id': 37.0, 'id': 2304.01196, 'source': 'http://arxiv.org/pdf/2304.01196', 'title': 'Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data'})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf5005",
   "metadata": {},
   "source": [
    "### Adding the Generation in RAG\n",
    "- 지금까진 쿼리로 구동되는 rag를 구축\n",
    "- 템플릿 생성 추가, 체인 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edf8613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
    "    contexts provided. If the question cannot be answered using the information\n",
    "    provided say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38ad8d",
   "metadata": {},
   "source": [
    "- contexts: 하나의 텍스트 조각으로 결합되며, 각 조각은 줄(---)로 구분되므로 모델은 정보의 한 비트가 끝나고 다른 정보가 시작되는 위치 파악가능 \n",
    "- \"\\n---\\n\": 각 텍스트 사이에 삽입되는 구분 기호 \n",
    "- 검색된 모든 문서를 가져와 단일 문자열로 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0f1fb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sy\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc/two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. These models outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety. The approach to fine-tuning and safety is detailed in the work.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = qa_chain(\n",
    "    inputs={\n",
    "        \"query\": question,\n",
    "        \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
    "    }\n",
    ")\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2378cb7",
   "metadata": {},
   "source": [
    "### Chaining Everything with a SequentialChain\n",
    "- 여러개의 체인을 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "847b98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
    "    docs = [d.page_content for d in docs]\n",
    "    docs_dict = {\n",
    "        \"query\": inputs[\"question\"],\n",
    "        \"contexts\": \"\\n---\\n\".join(docs)\n",
    "    }\n",
    "    return docs_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecf85d",
   "metadata": {},
   "source": [
    "- d.page_content: 텍스트 콘텐츠 추출, 검색된 각 문서에서 텍스트 콘텐츠(페이지 콘텐츠) 목록을 생성\n",
    "- context 결합: 모든 문서 텍스트를 단일 문자열을 \\n---\\n으로 구분하여 원래 쿼리와 함께 딕셔너리에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35a55022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc5794c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What information can you provide about llama 2?', 'What are some details about llama 2?', 'Can you share some insights on llama 2?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. The fine-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. These models outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety. The approach to fine-tuning and safety is detailed in the work.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d41ef7",
   "metadata": {},
   "source": [
    "- TransformChain(retrieval_chain)으로 쿼리와 관련된 문서를 검색하고 처리\n",
    "- SequentialChain으로 앞선 출력물(쿼리와 컨텍스트로 생성된 사전)을 qa_chain으로 전달하여 최종 답변을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59614d09",
   "metadata": {},
   "source": [
    "### Custom Multiquery\n",
    "- 프롬프트에 따른 다양한 답변 생성 비교 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a1f1a",
   "metadata": {},
   "source": [
    "- LineList 클래스 생성\n",
    "- ** pydantic: 데이터의 타입과 범위를 검증해주는 라이브러리, type annotation을 사용하여 검증과 setting 관리\n",
    "- ** field: 데이터 범위, 길이제한 등 설정 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2756db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c635a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on Large Language Models, Machine Learning, and related\n",
    "disciplines.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64f308",
   "metadata": {},
   "source": [
    "- prompttemplate 생성\n",
    "- llm 설정\n",
    "- llmchain 생성 \n",
    "- outputparser: 출력을 원하는 형식으로 구문 분석하는 프로세스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cfd6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e017f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35eba1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key features and capabilities of Large Language Models like GPT-3 and BERT?', '2. How does Machine Learning play a role in the development and advancement of Large Language Models?', '3. What are the ethical considerations and potential risks associated with deploying Large Language Models in various industries and applications?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\n",
    "    query=question\n",
    ")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3f9e6",
   "metadata": {},
   "source": [
    "- 질문과 유사한 문서 검색: 12개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4381796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='2 Related Work\\n2.1 Large Language Models\\nIn recent times, Large Language Models (LLMs) have garnered increasing attention for their exceptional performance in diverse natural language processing (NLP) tasks. Initially, transformer\\nmodels such as BERT [Devlin et al., 2019], GPT [Radford and Narasimhan, 2018], and T5 [Raffel\\net al., 2020] were developed with different pre-training objectives. However, the emergence of GPT3 [Brown et al., 2020], which scales up the number of model parameters and data size, showcases\\nsigniﬁcant zero-shot generalization abilities, enabling them to perform commendably on previously\\nunseen tasks. Consequently, numerous LLMs such as OPT [Zhang et al., 2022], BLOOM [Scao\\net al., 2022], PaLM [Chowdhery et al., 2022], and LLaMA [Touvron et al., 2023] are created, ushering in the success of LLMs. Additionally, Ouyang et al. [Ouyang et al., 2022] propose InstructGPT\\nby aligning human instruction and feedback with GPT-3. Furthermore, it has been applied to Chat2\\nGPT [OpenAI, 2022], which facilitates conversational interaction with humans by responding to a\\nbroad range of diverse and intricate queries and instructions.', metadata={'chunk-id': 6.0, 'id': 2304.14178, 'source': 'http://arxiv.org/pdf/2304.14178', 'title': 'mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality'}),\n",
       " Document(page_content='challenges described above) about how the development of large language models has unfolded thus far, including a\\nquantitative analysis of the increasing gap between academia and industry for large model development.\\nFinally, in Section 4 we outline policy interventions that may help concretely address the challenges we outline in\\nSections 2 and 3 in order to help guide the development and deployment of larger models for the broader social good.\\nWe leave some illustrative experiments, technical details, and caveats about our claims in Appendix A.\\n2 DISTINGUISHING FEATURES OF LARGE GENERATIVE MODELS\\nWe claim that large generative models (e.g., GPT-3 [ 11], LaMDA [ 78], Gopher [ 62], etc.) are distinguished by four\\nfeatures:\\n•Smooth, general capability scaling : It is possible to predictably improve the general performance of generative\\nmodels — their loss on capturing a specific, though very broad, data distribution — by scaling up the size of the\\nmodels, the compute used to train them, and the amount of data they’re trained on in the correct proportions.\\nThese proportions can be accurately predicted by scaling laws (Figure 1). We believe that these scaling laws\\nde-risk investments in building larger and generally more capable models despite the high resource costs and the\\ndifficulty of predicting precisely how well a model will perform on a specific task. Note, the harmful properties', metadata={'chunk-id': 9.0, 'id': 2202.07785, 'source': 'http://arxiv.org/pdf/2202.07785', 'title': 'Predictability and Surprise in Large Generative Models'}),\n",
       " Document(page_content='based on attention which substantially improves the efficiency of large-scale model training on TPU. Their first\\nmodel is called GPT [ 6], which is now widely used for text generation tasks. The same year, Google develops\\nBERT [ 7] based on bidirectional transformer. BERT consists of 340M parameters, trained on 3.3 billion words,\\nand is the current state of the art embedding model. The trend of using larger models and more training data\\ncontinues. By the time this paper is published, OpenAI’s latest GPT-3 model [ 8] contains 170 billion parameters,\\nand Google’s GShard [9] contains 600 billion parameters.\\nAlthough these gigantic models show very impressive performance on various NLP tasks, some researchers\\nargue that they do not really understand language and are not robust enough for many mission-critical domains\\n[10–14]. Recently, there is an growing interest in exploring neuro-symbolic hybrid models (e.g., [ 15–18]) to\\naddress some of the fundamental limitations of neural models, such as lack of grounding, being unable to perform\\nsymbolic reasoning, not interpretable. These works, although important, are beyond the scope of this paper.\\nWhile there are many good reviews and text books on text classification methods and applications in general', metadata={'chunk-id': 6.0, 'id': 2004.03705, 'source': 'http://arxiv.org/pdf/2004.03705', 'title': 'Deep Learning Based Text Classification: A Comprehensive Review'}),\n",
       " Document(page_content='On the Multilingual Capabilities of Very Large-Scale English Language\\nModels\\nJordi Armengol-Estapé, Ona de Gibert Bonet, and Maite Melero\\nText Mining Unit\\nBarcelona Supercomputing Center\\n{jordi.armengol,ona.degibert,maite.melero}@bsc.es\\nAbstract\\nGenerative Pre-trained Transformers (GPTs)\\nhave recently been scaled to unprecedented\\nsizes in the history of machine learning. These\\nmodels, solely trained on the language modeling objective, have been shown to exhibit\\noutstanding few-shot learning capabilities in a\\nnumber of different tasks. Nevertheless, aside\\nfrom anecdotal experiences, little is known regarding their multilingual capabilities, given\\nthe fact that the pre-training corpus is almost\\nentirely composed of English text. In this\\nwork, we investigate the multilingual skills of\\nGPT-3, focusing on one language that barely\\nappears in the pre-training corpus, Catalan,\\nwhich makes the results especially meaningful; we assume that our results may be relevant\\nfor other languages as well. We ﬁnd that the\\nmodel shows an outstanding performance, particularly in generative tasks, with predictable\\nlimitations mostly in language understanding\\ntasks but still with remarkable results given the\\nzero-shot scenario. We investigate its potential and limits in extractive question-answering', metadata={'chunk-id': 0.0, 'id': 2108.13349, 'source': 'http://arxiv.org/pdf/2108.13349', 'title': 'On the Multilingual Capabilities of Very Large-Scale English Language Models'}),\n",
       " Document(page_content='Other intriguing capabilities exhibited by large language models include, but are not limited to, free-form\\ngeneration of coherent, long-form text like news stories, generating responses with real-world knowledge,\\nas well as performing rudimentary mathematical operations.\\nThe rapid development of large language models in recent years has also been fueled by growth in computational resources, availability of large datasets and evolving software stacks. State-of-the-art supercomputing\\nclusters address the computation, memory and networking need of model training at this scale. Careful\\nprocessing of high-quality, high-volume and diverse datasets directly contributes to model performance in\\ndownstream tasks as well as model convergence. New approaches to numerical manipulation and training\\nrecipes were developed aiming at improved optimization efﬁciency and stability. However, to sustain the\\nseemingly exponential growth of model parameter size (see Figure 1), substantial progress in developing\\nnew methods, infrastructure and training capabilities is needed.\\nTraining such large models is challenging for two reasons. First, it is no longer possible to ﬁt the parameters\\nof these models in the memory of even the largest GPU. Second, the large number of compute operations\\n2\\nrequired can result in unrealistically long training times if special attention is not paid to concurrently optimizing the algorithms, software, and hardware stack. This calls for efﬁcient parallelism techniques scalable', metadata={'chunk-id': 3.0, 'id': 2201.1199, 'source': 'http://arxiv.org/pdf/2201.11990', 'title': 'Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model'}),\n",
       " Document(page_content='et al., 2019; Howard and Ruder, 2018; Radford et al., 2018a).\\n1.2.3. “Large” Language Models\\nTherecentupwindinLMresearchisrootedinthecapacitytoincreaseLMsizeintermsofnumberofparameters\\nandsizeoftrainingdata(Benderetal.,2021). TrainingmodelsonextremelylargedatasetssuchastheColossal\\nClean Crawl Corpus (C4) (Raﬀel et al., 2020) and WebText (Radford et al., 2018b) resulted in sequence\\nprediction systems with much more general applicability compared to the prior state-of-the-art (Brown et al.,\\n2020; Fedus et al., 2021; Rosset, 2020). These models also displayed greater few-shot and zero-shot learning\\ncapabilities compared to smaller LMs (Brown et al., 2020). These properties were found to greatly simplify the\\ndevelopment of task-speciﬁc LAs by reducing the adaptation process to prompt design (Zhang et al., 2021b).\\nThe insight that powerful sequence prediction systems could be created by scaling up the size of LMs and\\ntraining corpora motivated an upsurge in interest and investment in LM research by several AI research labs.\\n8\\n2. Classiﬁcation of harms from language\\nmodels', metadata={'chunk-id': 26.0, 'id': 2112.04359, 'source': 'http://arxiv.org/pdf/2112.04359', 'title': 'Ethical and social risks of harm from Language Models'}),\n",
       " Document(page_content='we urge up-to-date benchmarks for measuring unforeseen behaviors inside large language models. Without\\nbenchmarking the emergent abilities, it could be hard to\\nmitigate the risks and problems at scale. Secondly, we\\nnote that larger language models are generally trained\\nwith more data. Assuming the data is completely clean\\nand informatively correct, language models will still fail to\\nlearnallinformationandknowledge,andalsomaywrongly\\ncorrelate information to each other. Furthermore, under\\nthescopeofthefoundationmodels,multimodaldatacould\\nbring the possibility of miscorrelation between different\\nmodalities.\\nb) Machine Learning Data: Our discussion lies in the\\ncollection and usage of machine learning data. Previous\\nstudy [88] suggests that high-quality language data is\\nlikelyexhaustedbefore2026,andlow-qualitylanguageand\\nimage data could be run out by 2060. This implies that the\\nlimited progress of data collection and construction could\\nbe constraints of future LLM development. Furthermore,\\nas better-quality data is assumed to train language models\\nwith better performances, companies and independent\\nresearchers are spending more time on data curation.However, this can not be done easily under the lowresource and low-budget scenarios. Even if we pay much\\neffort to design comprehensive human annotation frameworks,thedatacouldstillcontaininaccurateormisleading', metadata={'chunk-id': 51.0, 'id': 2301.12867, 'source': 'http://arxiv.org/pdf/2301.12867', 'title': 'Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity'}),\n",
       " Document(page_content='large LMs in general. Hence, more concerted e\\x0borts should be pursued to provide scalable solutions that can\\nput guardrails against such malicious uses.\\nDeploying PaLM-Coder to assist software development has additional complications and ethical considerations,\\nwhich we discuss in Section 6.4. It is an open problem both to ensure that LM-based suggestions are correct,\\nrobust, safe, and secure, and to ensure that developers are con\\x0cdent that the suggestions have these properties.\\n12 Related Work\\nNatural language capabilities have signi\\x0ccantly advanced through large scale language modeling over the\\nlast several years. Broadly, language modeling refers to approaches for predicting either the next token in\\na sequence or for predicting masked spans (Devlin et al., 2019; Ra\\x0bel et al., 2020). These self-supervised\\nobjectives when applied to vast corpora including data scraped from the internet, books, and forums, have\\nresulted in models with advanced language understanding and generation capabilities. Predictable power-laws\\nof model quality through scaling the amount of data, parameters, and computation have made this a reliable\\napproach for increasingly more capable models (Kaplan et al., 2020).\\nThe Transformer architecture (Vaswani et al., 2017) unleashed unparalleled e\\x0eciency on modern accelerators\\nand has become the de-facto approach for language models. In the span of only four years, the largest', metadata={'chunk-id': 168.0, 'id': 2204.02311, 'source': 'http://arxiv.org/pdf/2204.02311', 'title': 'PaLM: Scaling Language Modeling with Pathways'}),\n",
       " Document(page_content='Ethical and social risks of harm from\\nLanguage Models\\nLaura Weidinger1, John Mellor1, Maribeth Rauh1, Conor Griﬃn1, Jonathan Uesato1, Po-Sen Huang1, Myra\\nCheng1,2, Mia Glaese1, Borja Balle1, Atoosa Kasirzadeh1,3, Zac Kenton1, Sasha Brown1, Will Hawkins1, Tom\\nStepleton1, Courtney Biles1, Abeba Birhane1,4, Julia Haas1, Laura Rimell1, Lisa Anne Hendricks1, William\\nIsaac1, Sean Legassick1, Geoﬀrey Irving1and Iason Gabriel1\\n1DeepMind,2California Institute of Technology,3University of Toronto,4University College Dublin\\nAbstract\\nThis paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In\\norder to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by\\nthese models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on\\nmultidisciplinary literature from computer science, linguistics, and social sciences.\\nThe paper outlines six speciﬁc risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III.', metadata={'chunk-id': 0.0, 'id': 2112.04359, 'source': 'http://arxiv.org/pdf/2112.04359', 'title': 'Ethical and social risks of harm from Language Models'}),\n",
       " Document(page_content='reasoning and question answering.\\nAdditionally, bias can pervade a system depending on the speci\\x0cc downstream application, its speci\\x0cc training\\npipeline, and application-level protections (e.g., safety \\x0clters). While we evaluate the pre-trained model\\nhere for fairness and toxicity along certain axes, it is possible that these biases can have varied downstream\\nimpacts depending on how the model is used. It is also unclear if evaluations done on the pre-trained language\\nmodels a\\x0bect the downstream task evaluations after the models are \\x0cnetuned. Therefore, we recommend\\nappropriate measures be taken to assess the fairness gaps in the application context before deployment.\\n11 Ethical Considerations\\nThe ability to do language modeling at such large scale and quality unlocks the potential for a wide variety\\nof real world applications, including in healthcare and education domains (Bommasani & et. al., 2021).\\n45\\nHowever, recent research has pointed out various potential risks associated with such large-scale generalpurpose language models trained on web text (Bender et al., 2021; Bommasani & et. al., 2021), for instance,\\nperpetuating or exacerbating social stereotypes and disparities that are re\\rected in training data (Sheng\\net al., 2021), memorizing and revealing private information (Carlini et al., 2022), or causing downstream', metadata={'chunk-id': 163.0, 'id': 2204.02311, 'source': 'http://arxiv.org/pdf/2204.02311', 'title': 'PaLM: Scaling Language Modeling with Pathways'}),\n",
       " Document(page_content='of LLMs, ranging from discovering unethical behavior to\\nmitigating bias [14]. Weidinger et al. [15] systematically\\nstructured the ethical risk landscape with LLMs, clearly\\nidentifying six risk areas: 1) Discrimination, Exclusion,\\nand Toxicity, 2) Information Hazards, 3) Misinformation\\nHarms, 4) Malicious Uses, 5) Human-Computer Interaction Harms, 6) Automation, Access, and Environmental\\nHarms. Although their debate serves as the foundation\\nfor NLP ethics research, there is no indication that all\\nhazards will occur in recent language model systems. Empirical evaluations [16, 17, 18, 19, 20] have revealed that\\nlanguage models face ethical issues in several downstream\\nactivities. Using exploratory studies via model inference,\\nadversarial robustness, and privacy, for instance, early\\nresearch revealed that dialogue-focused language models\\nposed possible ethical issues [21]. Several recent studies\\nhave demonstrated that LLMs, such as GPT-3, have a\\npersistent bias against genders [22] and religions [23].\\nExpectedly, LLMs may also encode toxicity, which results\\nin ethical harms. For instance, Si et al. [24] demonstrated\\nthat BlenderBot[25] and TwitterBot [26] can easily trigger\\ntoxic responses, though with low toxicity.', metadata={'chunk-id': 3.0, 'id': 2301.12867, 'source': 'http://arxiv.org/pdf/2301.12867', 'title': 'Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity'}),\n",
       " Document(page_content='extensively discussed and studied in previous works (e.g., [ 53,54]). Issues encountered include toxicity (e.g., [ 55,56,\\n57]), bias (e.g., [ 58,59,60,61,62,63,64,65,66,67,68,69,70,71,72]), and inappropriately revealing personally\\nidentifying information (PII) from training data [ 73]. Weidinger et al. [ 54] identify 21 risks associated with large-scale\\n3\\nlanguage models and discuss the points of origin for these risks. While many mitigation strategies have also been\\nsuggested (e.g., [ 74,75,76,77,78,79,80,81,82]), meaningfully addressing these issues remains an active research\\narea.\\nSimilar issues have also been discussed speciﬁcally for dialog models [ 53]. For instance, examples of bias, offensiveness,\\nand hate speech have been found both in training data drawn from social media, and consequently in the output of dialog\\nmodels trained on such data [ 83]. Dialog models [ 84] can learn, and even amplify, biases in the training data. Echoing\\nGehman et al. [ 85], we ﬁnd ﬁne-tuning effective to augment language models for safety. The method we use in this', metadata={'chunk-id': 12.0, 'id': 2201.08239, 'source': 'http://arxiv.org/pdf/2201.08239', 'title': 'LaMDA: Language Models for Dialog Applications'})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb1a5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ee9e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How do Large Language Models like GPT-3 contribute to advancements in natural language processing?', '2. What are the key differences between traditional machine learning algorithms and deep learning models used in language processing?', '3. What are the ethical considerations surrounding the use of Large Language Models in various industries and applications?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLaMA is a Large Language Model created by Touvron et al. in 2023, which is part of the success of Large Language Models (LLMs). It is one of the numerous LLMs like OPT, BLOOM, and PaLM that have been developed to showcase significant zero-shot generalization abilities and perform well on diverse natural language processing tasks.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab29e94",
   "metadata": {},
   "source": [
    "- 라마2에 대해 알려주세요\n",
    "- 'LLaMA는 Touvron 등이 2023년에 개발한 대형 언어 모델입니다. GPT-3와 같은 모델의 성공을 활용하기 위해 개발된 수많은 LLM 중 하나입니다. LLaMA는 OPT, BLOM 및 PaLM과 같은 다른 모델과 함께 중요한 제로 샷 일반화 능력을 보여주고 다양한 자연어 처리 작업에서 탁월한 것을 목표로 합니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19ce6ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives.\n",
    "Each query MUST tackle the question from a different viewpoint,\n",
    "we want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0535bf",
   "metadata": {},
   "source": [
    "- logging / class annotation / contexts 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff918e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
